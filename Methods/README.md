# Bayes Ridge Regression
Die Bayessche Ridge-Regression ist eine Variante der linearen Regression, bei der ein bayesscher Ansatz verwendet wird. Im Gegensatz zur klassischen Ridge-Regression liefert sie nicht nur eine Schätzung der Regressionsgewichte, sondern auch deren Unsicherheit (in Form von Wahrscheinlichkeitsverteilungen).

# Bayesian Regression
Bayesian Regression ist eine Erweiterung der klassischen Regressionsmethoden, bei der Wahrscheinlichkeiten (anstatt nur Punkt-Schätzungen) für die Modellparameter und Vorhersagen verwendet werden. Im Gegensatz zu Methoden wie der gewöhnlichen linearen Regression gibt Bayesian Regression Verteilungen über die Regressionsgewichte aus, nicht nur feste Werte.

# Decision Tree Classifier
Ein Decision Tree Classifier (Entscheidungsbaum-Klassifikator) ist ein überwacht lernendes Modell, das zur Klassifikation von Daten verwendet wird. Es stellt Entscheidungen in Form eines Baums dar, wobei jeder innere Knoten eine Entscheidung (eine Frage zu einem Merkmal) und jedes Blatt eine Klasse darstellt.

# Gaussian Regression
Gaussian Regression ist ein anderer Begriff für Gaussian Process Regression (GPR) – eine nicht-parametrische, bayessche Methode zur Regression. Im Gegensatz zu linearen Modellen sagt Gaussian Regression nicht nur einen Mittelwert vorher, sondern auch eine Verteilung über mögliche Ausgabewerte. Das macht sie besonders mächtig für unsichere oder nichtlineare Probleme.

# Hierarchical Clustering
Hierarchisches Clustering (Hierarchical Clustering) ist ein unsupervised learning-Verfahren zur Gruppierung (Clustering) von Datenpunkten, bei dem eine hierarchische Struktur in Form eines Baums (Dendrogramm) erstellt wird. Dabei müssen keine Anzahl von Clustern im Voraus angegeben werden – das Modell erzeugt eine Folge von Verschachtelungen.

# KNN Klassifier
Der k-Nächste-Nachbarn-Klassifikator (k-Nearest Neighbors Classifier, kurz kNN) ist ein einfaches, aber effektives Verfahren des überwachten Lernens, das zur Klassifikation (und auch Regression) verwendet wird.

# Kernel Ridge Regression
Kernel Ridge Regression ist eine Kombination aus Ridge Regression bei der eine lineare Regressionsmethode mit L2-Regularisierung (verhindert Overfitting) verwendet wird und dem Kernel-Trick - ein Verfahren, das es ermöglicht, nichtlineare Zusammenhänge zu modellieren, ohne die Daten explizit in einen höheren Raum zu transformieren.

# Lasso Interpolation
Lasso Regression (Least Absolute Shrinkage and Selection Operator) ist eine Variante der linearen Regression, bei der eine L1-Regularisierung hinzugefügt wird. Sie wird häufig verwendet, um Overfitting zu vermeiden und gleichzeitig eine Feature Selection (Merkmalsauswahl) durchzuführen, indem sie einige der Regressionskoeffizienten auf Null setzt.

# Linear Regression
Lineare Regression ist eine grundlegende Technik im überwachten Lernen, die verwendet wird, um den Zusammenhang zwischen einer oder mehreren unabhängigen Variablen (Features) und einer abhängigen Variablen (Zielvariable) zu modellieren. Ziel der linearen Regression ist es, eine lineare Beziehung zwischen den Variablen zu finden, um zukünftige Werte vorherzusagen.

# MLP Classifier
Der MLP Classifier (Multilayer Perceptron Classifier) ist ein künstliches neuronales Netzwerk und eine sehr mächtige Methode für die Klassifikation. MLPs gehören zur Gruppe der feedforward neural networks, bei denen die Eingabedaten durch mehrere Schichten von Neuronen fließen, die jeweils mit den vorherigen und nachfolgenden Schichten verbunden sind.

# PCA Dimensionality Reduction
Principal Component Analysis (PCA) ist eine Technik zur dimensionalen Reduktion, die häufig verwendet wird, um die Anzahl der Merkmale (Features) in einem Datensatz zu reduzieren, während möglichst viel Information erhalten bleibt. Dies wird erreicht, indem neue Variablen (Hauptkomponenten) erzeugt werden, die lineare Kombinationen der ursprünglichen Merkmale sind und die maximale Varianz im Datensatz erklären.

PCA hilft, die Komplexität der Daten zu reduzieren, indem weniger relevante oder redundant erscheinende Merkmale entfernt werden, was die Analyse und Visualisierung vereinfacht und den Rechenaufwand für nachfolgende Algorithmen verringert.

# PSL Regression
PSL Regression bezieht sich auf die Probabilistic Soft Logic (PSL) Regression, eine Methode aus dem Bereich des maschinellen Lernens und der probabilistischen Modellierung, die in erster Linie zur Modellierung von Unsicherheiten und zur Handhabung von unscharfen oder ungenauen Daten verwendet wird. Es ist eine Technik, die in logischen Modellen arbeitet und insbesondere bei der Regression von Daten zum Einsatz kommt.

# Polynomial Regression
Polynomiale Regression (Polynomial Regression) ist eine Erweiterung der linearen Regression, bei der ein polynomielles Modell anstelle eines linearen Modells verwendet wird, um die Beziehung zwischen den unabhängigen Variablen (Features) und der abhängigen Variablen (Zielvariable) zu beschreiben. Diese Methode wird verwendet, wenn die Beziehung zwischen den Variablen nicht linear, sondern krumm oder gekürzt ist.

# Ridge Classifier
Der Ridge Classifier ist eine Variante des linearen Klassifikators, der eine Regularisierungstechnik namens Ridge Regression (auch als Tikhonov-Regularisierung bekannt) anwendet, um die Modelle zu stabilisieren und Überanpassung (Overfitting) zu vermeiden. Dieser Klassifikator verwendet die Ridge Regression für die Klassifizierung von Daten und wird häufig in Szenarien eingesetzt, in denen es wichtig ist, die Komplexität des Modells zu kontrollieren.

# SVM Linear Kernel
Die Support Vector Machine (SVM) ist ein äußerst leistungsfähiger Klassifikator, der in vielen maschinellen Lernaufgaben verwendet wird. Die Grundidee der SVM besteht darin, eine hyperebene zu finden, die die Datenpunkte einer bestimmten Klasse von den Datenpunkten einer anderen Klasse trennt. Der lineare Kernel ist dabei der einfachste Fall, bei dem die Trennung zwischen den Klassen durch eine gerade Linie (in 2D) oder eine flache Hyperebene (in höheren Dimensionen) erfolgt.

# Spectral Clustering
Spectral Clustering ist eine Technik des unsupervised learning, die in der Clusteranalyse verwendet wird. Sie basiert auf der Eigenwertzerlegung der Laplacian-Matrix einer Graphenstruktur. Im Gegensatz zu anderen Clustering-Algorithmen wie k-Means verwendet Spectral Clustering die Struktur der Daten, um die Ähnlichkeiten und Beziehungen zwischen den Punkten zu erfassen. Spectral Clustering ist besonders nützlich, wenn die Cluster in den Daten nicht-konvex oder nicht-linear sind, was traditionelle Methoden wie k-Means erschwert.

# Stochastic Gradient Descent
Stochastic Gradient Descent (SGD) ist eine Optimierungsmethode, die häufig in maschinellen Lernverfahren, insbesondere in linearen Regressionsmodellen, logistischen Regressionsmodellen und neuronalen Netzwerken, verwendet wird. Es ist eine Variante des klassischen Gradient Descent-Algorithmus, die darauf abzielt, das Optimierungsproblem schneller und effizienter zu lösen, indem es nur einen einzelnen Datenpunkt bei jedem Schritt verwendet, anstatt den gesamten Datensatz.

# Support Vector Classifier
Der Support Vector Classifier (SVC) ist ein Modell des überwachten Lernens, das auf der Methode der Support Vector Machines (SVMs) basiert. SVMs sind eine der leistungsstärksten und beliebtesten Methoden für Klassifikationsaufgaben und werden sowohl in einfachen als auch in komplexen Szenarien verwendet. Die Grundidee von SVC ist, einen hyperplane (Hyperplane) in einem hochdimensionalen Raum zu finden, der die Klassen optimal trennt. Der Hyperplane ist die Grenze, die die Datenpunkte in verschiedene Klassen unterteilt. SVC zielt darauf ab, diesen Hyperplane so zu positionieren, dass der Abstand (Margin) zwischen den nächstgelegenen Datenpunkten jeder Klasse maximiert wird.
